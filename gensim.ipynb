{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Gensim Library\n",
    "\n",
    "The following notebook will be an exercise in applying methods from the gensim library to the Blog Spot homeless blog corpus.\n",
    "\n",
    "The aim is to process data mined using the `blogspot_scraper` which can be used to model a classification system or aid in the statistical analysis of textual information. The hope is that it could useful for creating a framework that can be useful in predicting successful outcomes, or evaluating sentiment scores of posts, comments or writings of people dealing with homelessness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     wanderingscribe  \\\n",
       "0  # Extracted from http://wanderingscribe.blogsp...   \n",
       "1  In case you were wondering... the paperback of...   \n",
       "2  December probably isn't the time for it, but I...   \n",
       "3  Sometimes I give in to dreams —  dream that on...   \n",
       "4  In the meantime though, it's hard graft and sc...   \n",
       "\n",
       "                           homelesschroniclesintampa  \\\n",
       "0  #IWSG - JANUARY 2019 - CHECK IN - A NEW ME??? ...   \n",
       "1                                             Gee,\\n   \n",
       "2  this is a great question, and before I ever wr...   \n",
       "3  play the viola. Then, I came down with essenti...   \n",
       "4  neuro-muscular disorder (my mom was afflicted ...   \n",
       "\n",
       "                       livinghomelessourwritetospeak  \\\n",
       "0                                      Another one\\n   \n",
       "1  Its been some time since I actually have writt...   \n",
       "2  I am presently having a great meal as I write ...   \n",
       "3  Back when I last posted, I was running a new b...   \n",
       "4  I actually had myself a big slip and started u...   \n",
       "\n",
       "                                     seattlehomeless  \\\n",
       "0                                 # Archived posts\\n   \n",
       "1  # Extracted from https://seattlehomeless.blogs...   \n",
       "2  Ok, it's all relative.  Seattle is hot at 80 d...   \n",
       "3  So tonight one of our local politicians, Seatt...   \n",
       "4                            Here's what he saw:  \\n   \n",
       "\n",
       "                                            homevan  \\\n",
       "0                     HOME VAN NEWSLETTER 6/12/16\\n   \n",
       "1                     HOME VAN NEWSLETTER 1/18/16\\n   \n",
       "2  HOLIDAY ANGELS DISGUISED AS HOMELESS STRANGERS\\n   \n",
       "3                    HOME VAN NEWSLETTER 11/15/15\\n   \n",
       "4                     HOME VAN NEWSLETTER 10/4/15\\n   \n",
       "\n",
       "                                         joe-anybody  \\\n",
       "0                        House Keys Not Handcuffs \\n   \n",
       "1                      A (sticker) and a good idea\\n   \n",
       "2                                  - Portland 2018\\n   \n",
       "3                                           - WRAP\\n   \n",
       "4  On 9/28/15 in Portland Oregon I filmed this in...   \n",
       "\n",
       "                                    thehomelessfinch  \n",
       "0  The Homeless Finch Has Found Her Nest: Project...  \n",
       "1       The Homeless Finch Makes It's First Rescue\\n  \n",
       "2  The Start of Something New for The Homeless Fi...  \n",
       "3  Jehane Lyle, Watercolor on paper, \"Cuppa\" - de...  \n",
       "4  This week has been a complete blast.  Getting ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wanderingscribe</th>\n      <th>homelesschroniclesintampa</th>\n      <th>livinghomelessourwritetospeak</th>\n      <th>seattlehomeless</th>\n      <th>homevan</th>\n      <th>joe-anybody</th>\n      <th>thehomelessfinch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td># Extracted from http://wanderingscribe.blogsp...</td>\n      <td>#IWSG - JANUARY 2019 - CHECK IN - A NEW ME??? ...</td>\n      <td>Another one\\n</td>\n      <td># Archived posts\\n</td>\n      <td>HOME VAN NEWSLETTER 6/12/16\\n</td>\n      <td>House Keys Not Handcuffs \\n</td>\n      <td>The Homeless Finch Has Found Her Nest: Project...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In case you were wondering... the paperback of...</td>\n      <td>Gee,\\n</td>\n      <td>Its been some time since I actually have writt...</td>\n      <td># Extracted from https://seattlehomeless.blogs...</td>\n      <td>HOME VAN NEWSLETTER 1/18/16\\n</td>\n      <td>A (sticker) and a good idea\\n</td>\n      <td>The Homeless Finch Makes It's First Rescue\\n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>December probably isn't the time for it, but I...</td>\n      <td>this is a great question, and before I ever wr...</td>\n      <td>I am presently having a great meal as I write ...</td>\n      <td>Ok, it's all relative.  Seattle is hot at 80 d...</td>\n      <td>HOLIDAY ANGELS DISGUISED AS HOMELESS STRANGERS\\n</td>\n      <td>- Portland 2018\\n</td>\n      <td>The Start of Something New for The Homeless Fi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sometimes I give in to dreams —  dream that on...</td>\n      <td>play the viola. Then, I came down with essenti...</td>\n      <td>Back when I last posted, I was running a new b...</td>\n      <td>So tonight one of our local politicians, Seatt...</td>\n      <td>HOME VAN NEWSLETTER 11/15/15\\n</td>\n      <td>- WRAP\\n</td>\n      <td>Jehane Lyle, Watercolor on paper, \"Cuppa\" - de...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>In the meantime though, it's hard graft and sc...</td>\n      <td>neuro-muscular disorder (my mom was afflicted ...</td>\n      <td>I actually had myself a big slip and started u...</td>\n      <td>Here's what he saw:  \\n</td>\n      <td>HOME VAN NEWSLETTER 10/4/15\\n</td>\n      <td>On 9/28/15 in Portland Oregon I filmed this in...</td>\n      <td>This week has been a complete blast.  Getting ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('blog_spot.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing and cleaning\n",
    "\n",
    "The text data is arranged as elements of a dataframe. Since this will be used as unsupervised training data. We can combine the writings of our sample into one series object to make mapping easier. Unless someone wants to explicitly point it out, I do not anticipate computational problems arising from this. Standard data processing and cleaning methods are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim .utils import simple_preprocess\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in function\n",
    "df1 = df.apply(lambda x: ','.join(x.astype(str)), axis=1)\n",
    "df2 = df1.apply(lambda x: remove_stopwords(x))\n",
    "df3 = df2.apply(lambda x: simple_preprocess(x, min_len=2))\n",
    "extra_stop_words = [\"http\", \"like\"] # extra_stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for i in extra_stop_words:\n",
    "    stop_words.append(i)\n",
    "\n",
    "df3a = df3.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df4 = df3a.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing processed data\n",
    "Below is a comparison of the before and after states of the text. The output in `df4` can now be converted into a bag-of-words (`bow`).\n",
    "\n",
    "Before:\n",
    "\n",
    "'# Extracted from http://wanderingscribe.blogspot.com/\\n,#IWSG - JANUARY 2019 - CHECK IN - A NEW ME??? I CERTAINLY HOPE SO!\\n,Another one\\n,# Archived posts\\n,HOME VAN NEWSLETTER 6/12/16\\n,House Keys Not Handcuffs\\xa0\\n,The Homeless Finch Has Found Her Nest: Projects, Plans and Peace\\n'\n",
    "\n",
    "After:\n",
    "\n",
    "['extracted', 'wanderingscribe', 'blogspot', 'iwsg',...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign unique ID\n",
    "Now that we have a list representing the dictionary of words found in our corpus, we can create a dictionary of `{key: word, value: frequency_count}` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The word homeless appears 1971 times in the corpus\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "# Create a frequency count for words in dataframe\n",
    "for text in df4:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Sample frequency count\n",
    "word = 'homeless'\n",
    "print(\"The word {} appears {} times in the corpus\".format(word, frequency[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words\n",
    "Using the `collections` module, we can see what the top 10 words by frequency count are.  We can also use this to further remove non-content words from the corpus if necessary. This is how I found that the word 'http' was everywhere in the corpus. I've removed it since, but it gives you an idea of non-content words you might have missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('nan', 5232),\n",
       " ('homeless', 1971),\n",
       " ('people', 1803),\n",
       " ('night', 1048),\n",
       " ('time', 934),\n",
       " ('nightwatch', 860),\n",
       " ('know', 807),\n",
       " ('going', 770),\n",
       " ('shelter', 750),\n",
       " ('got', 687)]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(frequency)\n",
    "c.most_common(10)"
   ]
  },
  {
   "source": [
    "### Word appearing n times\n",
    "I thought it might be a good idea to find words appearing n number of times in the corpus. You can also do the exact opposite and find words that appear less than n times in the corpus and decide whether you want to keep those. This makes sense (as the Gensim docs point out) when you're using the library on massive datasets like the google news dataset - in a corpus of a billion words, you might want to decide that words appearing less than 5 times can be removed, for example."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['blogspot', 'com', 'iwsg', 'january', 'check', 'new', 'certainly', 'hope', 'another', 'posts', 'home', 'van', 'newsletter', 'house', 'keys', 'homeless', 'finch', 'found', 'projects', 'plans', 'peace'], ['case', 'wondering', 'book', 'new', 'cover', 'th', 'november', 'cover', 'looks', 'sure', 'pink', 'white', 'time', 'actually', 'written', 'post', 'blog', 'figured', 'time', 'update', 'readers', 'past', 'year', 'bit', 'blogspot', 'com', 'home', 'van', 'newsletter', 'good', 'idea', 'homeless', 'finch', 'makes', 'first', 'rescue']]\n"
     ]
    }
   ],
   "source": [
    "# output words appearing at least n times\n",
    "n = 10\n",
    "processed_corpus = [[token for token in text if frequency[token] > n] for text in df4]\n",
    "print(processed_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary keys\n",
    "Now that we have a list of all the words in the corpus, we can create a dictionary to map any future comparison to. These will be assigned a unique token_id for every token or word in the corpus. What we'll end up with is a bag-of-words with unique identifiers for words in our entire corpus. Note the difference between `processed_corpus` and `bow_corpus`, where the former is a untokenized list of context words in our corpus. For this, we will use the built-in `corpora` module from the `gensim` library to make things easy.\n",
    "\n",
    "Sample output:\n",
    "\n",
    "dictionary.token2id:\n",
    "{'check': 0, 'home': 1, 'homeless': 2, 'hope': 3...\n",
    "\n",
    "bow_corpus:\n",
    "[(2, 4), (3, 1), (4, 1), (5, 2), (6, 9)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create (token, tokenID)\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "# bow representation of the corpus; (tokenID, freq)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply doc2bow to example_doc\n",
    "Now we can test whether we can identify words present in an `example_doc` that are also in our `bow_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The words: ['day', 'happy', 'nice'] are in our dictionary. No biggie.\n"
     ]
    }
   ],
   "source": [
    "# Modify for testing\n",
    "example_doc = \"it's a nice and sunny day outside, i'm so happy\"\n",
    "\n",
    "# Finds words in example_doc that appear within the context of our corpus\n",
    "example_bow = dictionary.doc2bow(example_doc.lower().split())\n",
    "example_bow\n",
    "\n",
    "if len(example_bow) == 0:\n",
    "    print(\"No context word found.\")\n",
    "else:\n",
    "    print(f\"The words: {[dictionary[tokenID] for tokenID, freq in example_bow]} are in our dictionary. No biggie.\")\n",
    "\n",
    "# TODO - Output (token, token_freq) in example_doc for words in corpora.Dictionary (processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "## TF-IDF weight\n",
    "In order to quantify the relationship of words in a document (and across documents), we'll create a word embeddings using `tf-idf` weights using the bow_corpus. We'll then transform `example_doc2` into a bag-of-words. This way, we will be able to determine which words in our `example_doc2` also appear in our training data, and actually measure both the frequency of those words in a document, and across documents. This is measured by it's tf-idf weight and helps answer the question: \"How relevant are words that appear in a document across a collection of documents\". The wikipedia article on tf-idf probably does a better job at explaining this. Later we will use a different algorithm (LSI) to calculate the similarity of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Initialize tf-idf model using bow_corpus\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "source": [
    "### bow representation to tf-idf weights\n",
    "This gives us a measure of the frequency of a word (i.e. assumed relevance) given a corpus of documents.\n",
    "\n",
    "Sample tfidf[dictionary.doc2bow(sample)] output:\n",
    "\n",
    "[(0, 0.017154431730943658), (1, 0.020673564600620076), (2, 0.033339482607518046)..."
   ],
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with manual document\n",
    "We can test how well the model is able to determine an input's similiarity, measured in terms of tf-idf weights, by feeding it some text manually and seeing how it compares to the transformed corpus.\n",
    "\n",
    "The output gives you the `(dictionary[key], tf-idf_weight)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tfidf weight of unrelated_doc: \n[(494, 0.5792698834554537), (1630, 0.5662128359240098), (2361, 0.5863867550997339)]\n\nTfidf weight of related_doc: \n[(8, 0.15950015416535915), (191, 0.3193789464903235), (241, 0.30415827237259907), (353, 0.6306102318406809), (1234, 0.6183649975581706)]\n\n"
     ]
    }
   ],
   "source": [
    "example_doc1 = \"I am a student and I like to study natural language processing and computer science.\"\n",
    "unrelated_doc = example_doc1.lower().split()\n",
    "unrelated_doc_dict = dictionary.doc2bow(unrelated_doc)\n",
    "unrelated_doc_tfidf = tfidf[unrelated_doc_dict]\n",
    "\n",
    "example_doc2 = \"As a homeless person, I need all the help I can get from the police, instead of them harrassing me.\"\n",
    "related_doc = example_doc2.lower().split()\n",
    "related_doc_dict = dictionary.doc2bow(related_doc)\n",
    "related_doc_tfidf = tfidf[related_doc_dict]\n",
    "\n",
    "print(f\"Tfidf weight of unrelated_doc: \\n{unrelated_doc_tfidf}\\n\")\n",
    "print(f\"Tfidf weight of related_doc: \\n{related_doc_tfidf}\\n\")\n",
    "\n",
    "# TODO - create clean output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to disk\n",
    "TODO\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
    "    lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
    "\n",
    "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "source": [
    "### Applying the LSI (chain) model to the corpus.\n",
    "An interesting observation here is the difference in results obtained when applying LSI on top of either: a bag-of-words representation of the corpus (`bow_corpus`) or a tf-idf transformation (`corpus_tfidf`) on top of the bow_corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics to generate\n",
    "NUM_TOPICS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Topics generated with BOW:\n",
      "[(0, '0.411*\"homeless\" + 0.341*\"people\" + 0.257*\"nan\" + 0.180*\"city\" + 0.151*\"night\" + 0.148*\"time\" + 0.146*\"shelter\" + 0.126*\"know\" + 0.125*\"life\" + 0.116*\"going\"'), (1, '0.805*\"nan\" + -0.270*\"homeless\" + -0.178*\"city\" + -0.142*\"police\" + -0.120*\"people\" + -0.107*\"pam\" + 0.100*\"night\" + 0.099*\"nightwatch\" + -0.084*\"homelessness\" + -0.082*\"law\"')]\n",
      "\n",
      "Topics generated with tf-idf weights:\n",
      "[(0, '0.155*\"homeless\" + 0.149*\"people\" + 0.136*\"night\" + 0.122*\"nightwatch\" + 0.109*\"shelter\" + 0.105*\"time\" + 0.104*\"seattle\" + 0.102*\"going\" + 0.098*\"know\" + 0.098*\"city\"'), (1, '-0.229*\"night\" + -0.223*\"shelter\" + -0.212*\"nightwatch\" + -0.184*\"seattle\" + -0.178*\"women\" + -0.157*\"bar\" + 0.150*\"life\" + -0.115*\"city\" + -0.109*\"operation\" + 0.104*\"justice\"')]\n"
     ]
    }
   ],
   "source": [
    "# BOW -> LSI\n",
    "lsi_bow = models.LsiModel(bow_corpus, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "\n",
    "print(f\"\\nTopics generated with BOW:\\n{lsi_bow.show_topics()}\") # TODO - drop NAN\n",
    "\n",
    "# BOW -> TF-IDF -> LSI \n",
    "corpus_tfidf = tfidf[bow_corpus] # bow_corpus as vector of tf-idf weights\n",
    "lsi_tfidf = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "\n",
    "print(f\"\\nTopics generated with TF-IDF:\\n{lsi_tfidf.show_topics()}\")"
   ]
  },
  {
   "source": [
    "Aside from having to do some more clean up, it's interesting to note the difference in words included in the generated topic by the LSI model. I'd have to dig deeper as to why, but based on what I've understood, the difference could be coming from the factoring in of term frequencies across documents (the idf part) which effectively penalizes the model for words that occur too frequent in the corpus, but have no semantic meaning within a document.\n",
    "\n",
    "Considering that the corpus is comprised of homeless blogs for example, one has to ask whether the word `homeless` actually carries much meaning in the corpus? Remember that BOW simply does a frequency count of words in a document, but does not account for justifiably meaningless words (functional words) that appear too frequent across the collection of documents. TF-IDF weights does exactly that - it adjusts (penalizes) for words that are too common to bear actual meaning (context words) for the document."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi_model[corpus_tfidf]) # tf-idf weights of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dictionary.doc2bow(example_doc.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lsi = lsi_model[texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_lsi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[test_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_sp = []\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims:\n",
    "    doc_sp.append([doc_score, documents[doc_position]]) # documents = list of sentences; documents[doc_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sims[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_corpus, window=5, min_count=10, workers=4) # Omit: vector_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(['hello','world'], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['homeless']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vocab['homeless'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2 = KeyedVectors.load('word2vec.model', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = wv2.__getitem__(['homeless'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tf = []\n",
    "for ID, tf in sims:\n",
    "    sample_tf.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_tf, 'ob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(sample_tf), 'ob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = [item for item in sims]\n",
    "#print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cos_sim, '+') # Does not mean anything! Clusters; K-nearest neighbor?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common_list = c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(c.most_common_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ([[word] for word, count in c.most_common_list])\n",
    "y = ([[count] for word, count in c.most_common_list])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}