{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF using Gensim \n",
    "\n",
    "The following notebook will be an exercise in applying methods from the gensim library to the Blog Spot homeless blog corpus.\n",
    "\n",
    "The aim is to process data that can be used to model a classification system or aid in the statistical analysis of textual information. The model can be applied to predicting the successful exit from homelessness, or evaluating sentiment scores for people on the brink of becoming homeless, depending on the theoretical framework used in examining the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     wanderingscribe  \\\n",
       "0  # Extracted from http://wanderingscribe.blogsp...   \n",
       "1  In case you were wondering... the paperback of...   \n",
       "2  December probably isn't the time for it, but I...   \n",
       "3  Sometimes I give in to dreams —  dream that on...   \n",
       "4  In the meantime though, it's hard graft and sc...   \n",
       "\n",
       "                           homelesschroniclesintampa  \\\n",
       "0  #IWSG - JANUARY 2019 - CHECK IN - A NEW ME??? ...   \n",
       "1                                             Gee,\\n   \n",
       "2  this is a great question, and before I ever wr...   \n",
       "3  play the viola. Then, I came down with essenti...   \n",
       "4  neuro-muscular disorder (my mom was afflicted ...   \n",
       "\n",
       "                       livinghomelessourwritetospeak  \\\n",
       "0                                      Another one\\n   \n",
       "1  Its been some time since I actually have writt...   \n",
       "2  I am presently having a great meal as I write ...   \n",
       "3  Back when I last posted, I was running a new b...   \n",
       "4  I actually had myself a big slip and started u...   \n",
       "\n",
       "                                     seattlehomeless  \\\n",
       "0                                 # Archived posts\\n   \n",
       "1  # Extracted from https://seattlehomeless.blogs...   \n",
       "2  Ok, it's all relative.  Seattle is hot at 80 d...   \n",
       "3  So tonight one of our local politicians, Seatt...   \n",
       "4                            Here's what he saw:  \\n   \n",
       "\n",
       "                                            homevan  \\\n",
       "0                     HOME VAN NEWSLETTER 6/12/16\\n   \n",
       "1                     HOME VAN NEWSLETTER 1/18/16\\n   \n",
       "2  HOLIDAY ANGELS DISGUISED AS HOMELESS STRANGERS\\n   \n",
       "3                    HOME VAN NEWSLETTER 11/15/15\\n   \n",
       "4                     HOME VAN NEWSLETTER 10/4/15\\n   \n",
       "\n",
       "                                         joe-anybody  \\\n",
       "0                        House Keys Not Handcuffs \\n   \n",
       "1                      A (sticker) and a good idea\\n   \n",
       "2                                  - Portland 2018\\n   \n",
       "3                                           - WRAP\\n   \n",
       "4  On 9/28/15 in Portland Oregon I filmed this in...   \n",
       "\n",
       "                                    thehomelessfinch  \n",
       "0  The Homeless Finch Has Found Her Nest: Project...  \n",
       "1       The Homeless Finch Makes It's First Rescue\\n  \n",
       "2  The Start of Something New for The Homeless Fi...  \n",
       "3  Jehane Lyle, Watercolor on paper, \"Cuppa\" - de...  \n",
       "4  This week has been a complete blast.  Getting ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wanderingscribe</th>\n      <th>homelesschroniclesintampa</th>\n      <th>livinghomelessourwritetospeak</th>\n      <th>seattlehomeless</th>\n      <th>homevan</th>\n      <th>joe-anybody</th>\n      <th>thehomelessfinch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td># Extracted from http://wanderingscribe.blogsp...</td>\n      <td>#IWSG - JANUARY 2019 - CHECK IN - A NEW ME??? ...</td>\n      <td>Another one\\n</td>\n      <td># Archived posts\\n</td>\n      <td>HOME VAN NEWSLETTER 6/12/16\\n</td>\n      <td>House Keys Not Handcuffs \\n</td>\n      <td>The Homeless Finch Has Found Her Nest: Project...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In case you were wondering... the paperback of...</td>\n      <td>Gee,\\n</td>\n      <td>Its been some time since I actually have writt...</td>\n      <td># Extracted from https://seattlehomeless.blogs...</td>\n      <td>HOME VAN NEWSLETTER 1/18/16\\n</td>\n      <td>A (sticker) and a good idea\\n</td>\n      <td>The Homeless Finch Makes It's First Rescue\\n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>December probably isn't the time for it, but I...</td>\n      <td>this is a great question, and before I ever wr...</td>\n      <td>I am presently having a great meal as I write ...</td>\n      <td>Ok, it's all relative.  Seattle is hot at 80 d...</td>\n      <td>HOLIDAY ANGELS DISGUISED AS HOMELESS STRANGERS\\n</td>\n      <td>- Portland 2018\\n</td>\n      <td>The Start of Something New for The Homeless Fi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sometimes I give in to dreams —  dream that on...</td>\n      <td>play the viola. Then, I came down with essenti...</td>\n      <td>Back when I last posted, I was running a new b...</td>\n      <td>So tonight one of our local politicians, Seatt...</td>\n      <td>HOME VAN NEWSLETTER 11/15/15\\n</td>\n      <td>- WRAP\\n</td>\n      <td>Jehane Lyle, Watercolor on paper, \"Cuppa\" - de...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>In the meantime though, it's hard graft and sc...</td>\n      <td>neuro-muscular disorder (my mom was afflicted ...</td>\n      <td>I actually had myself a big slip and started u...</td>\n      <td>Here's what he saw:  \\n</td>\n      <td>HOME VAN NEWSLETTER 10/4/15\\n</td>\n      <td>On 9/28/15 in Portland Oregon I filmed this in...</td>\n      <td>This week has been a complete blast.  Getting ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('blog_spot.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing and cleaning\n",
    "\n",
    "The text data is arranged as elements of a dataframe. Since this will be used as training data, we can combine the writings of our sample into one series object to make mapping easier. Standard data processing and cleaning methods are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim .utils import simple_preprocess\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in function\n",
    "df1 = df.apply(lambda x: ','.join(x.astype(str)), axis=1)\n",
    "df2 = df1.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'# Extracted http://wanderingscribe.blogspot.com/ ,#IWSG - JANUARY 2019 - CHECK IN - A NEW ME??? I CERTAINLY HOPE SO! ,Another ,# Archived posts ,HOME VAN NEWSLETTER 6/12/16 ,House Keys Not Handcuffs ,The Homeless Finch Has Found Her Nest: Projects, Plans Peace'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.apply(lambda x: simple_preprocess(x, min_len=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra_stop_words using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stop_words = [\"http\", \"like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in extra_stop_words:\n",
    "    stop_words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3a = df3.apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3a.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing processed data\n",
    "Below is a comparison of the before and after states of the text. The output in `df4` can now be converted into a bag-of-words (`bow`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'# Extracted from http://wanderingscribe.blogspot.com/\\n,#IWSG - JANUARY 2019 - CHECK IN - A NEW ME??? I CERTAINLY HOPE SO!\\n,Another one\\n,# Archived posts\\n,HOME VAN NEWSLETTER 6/12/16\\n,House Keys Not Handcuffs\\xa0\\n,The Homeless Finch Has Found Her Nest: Projects, Plans and Peace\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df1.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['extracted',\n",
       " 'wanderingscribe',\n",
       " 'blogspot',\n",
       " 'iwsg',\n",
       " 'january',\n",
       " 'check',\n",
       " 'certainly',\n",
       " 'hope',\n",
       " 'another',\n",
       " 'archived',\n",
       " 'posts',\n",
       " 'home',\n",
       " 'newsletter',\n",
       " 'house',\n",
       " 'keys',\n",
       " 'handcuffs',\n",
       " 'homeless',\n",
       " 'finch',\n",
       " 'found',\n",
       " 'nest',\n",
       " 'projects',\n",
       " 'plans',\n",
       " 'peace']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df4[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign unique ID\n",
    "Now that we have a list representing the dictionary of words found in the sample of writings (as list of lists per document), our soon to be `bow`, we can process the dictionary with the text as key and the frequency count as its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The word http appears 0 times in the corpus\nIt needs to be removed!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "# Create a frequency count for words in dataframe\n",
    "for text in df4:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Sample frequency count\n",
    "word = 'http'\n",
    "print(\"The word {} appears {} times in the corpus\".format(word, frequency[word]))\n",
    "print(\"It needs to be removed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words\n",
    "Using the `collections` module, we can see what the top 10 words by frequency count are. Note that further removal of non-content words from the corpus is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('homeless', 1971),\n",
       " ('people', 1803),\n",
       " ('night', 1048),\n",
       " ('time', 934),\n",
       " ('nightwatch', 860),\n",
       " ('know', 807),\n",
       " ('going', 770),\n",
       " ('shelter', 750),\n",
       " ('city', 677),\n",
       " ('little', 664)]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(frequency)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['check', 'hope', 'home', 'house', 'homeless'], ['book', 'sure', 'white', 'time', 'actually', 'post', 'blog', 'time', 'past', 'year', 'home', 'good', 'idea', 'homeless', 'first']]\n"
     ]
    }
   ],
   "source": [
    "# output words appearing at least n times\n",
    "n = 100\n",
    "processed_corpus = [[token for token in text if frequency[token] > n] for text in df4]\n",
    "print(processed_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary keys\n",
    "Now that we have a list of all the words in the corpus, we can create a dictionary to map any future comparison to. These will be assigned a unique tokenID for every token or word in the corpus. What we'll end up with is a bag-of-words with unique identifiers for our entire corpus. Note the difference between `processed_corpus` and `bow_corpus`, where the former is a untokenized list of context words in our corpus. For this, we will use the built-in `corpora` module from the `gensim` library to make things easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create (token, tokenID)\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "# bow representation of the corpus; (tokenID, freq)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "source": [
    "### Sample output\n",
    "dictionary.token2id:\n",
    "{'check': 0, 'home': 1, 'homeless': 2, 'hope': 3...\n",
    "\n",
    "bow_corpus:\n",
    "[(2, 4), (3, 1), (4, 1), (5, 2), (6, 9)..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply doc2bow to example_doc\n",
    "Now we can test whether we can identify words present in an `example_doc` that are also in our `bow_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify for testing\n",
    "example_doc = \"it's a nice and sunny day outside, i'm so happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The words: ['happy', 'nice'] are in our dictionary. No biggie.\n"
     ]
    }
   ],
   "source": [
    "# Output (token, token_freq) in example_doc for words in corpora.Dictionary (processed_corpus)\n",
    "# Finds words in example_doc that appear within the context of our corpus\n",
    "example_bow = dictionary.doc2bow(example_doc.lower().split())\n",
    "example_bow\n",
    "\n",
    "if len(example_bow) == 0:\n",
    "    print(\"No context word found.\")\n",
    "else:\n",
    "    print(f\"The words: {[dictionary[tokenID] for tokenID, freq in example_bow]} are in our dictionary. No biggie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "## TF-IDF weight\n",
    "In order to quantify the relationship of words in a document across documents, we'll create a `tf-idf` model using the bow_corpus as training data. We'll then transform `example_doc2` into a bag-of-words. This way, we will be able to determine which words in our `example_doc2` also appear in our training model. This is measured by it's tf-idf weight and helps answer the question: \"How relevant are words that appear in a document across a collection of documents\". Later we will use a different algorithm (LSI) to calculate the similarity of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Initialize tf-idf model using bow_corpus\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "source": [
    "### Transform bow representation into tf-idf weight of words\n",
    "This gives us a measure of the frequency of a word (i.e. assumed relevance) given a corpus of documents.\n",
    "\n",
    "Sample tfidf[dictionary.doc2bow(sample)] output:\n",
    "\n",
    "[(0, 0.017154431730943658), (1, 0.020673564600620076), (2, 0.033339482607518046)..."
   ],
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with manual document\n",
    "We can test how well the model is able to determine an input's similiarity, measured in terms of term frequency weights, by feeding it some text manually and seeing how it compares to the transformed corpus.\n",
    "\n",
    "`example_doc1` = \"i made significant stock investments last year\"\n",
    "\n",
    "`example_doc2` = \"As a homeless person, I need all the help I can get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc1 = \"I am a student and like to study natural language processing\"\n",
    "unrelated_doc = example_doc1.lower().split()\n",
    "unrelated_doc_dict = dictionary.doc2bow(unrelated_doc)\n",
    "unrelated_doc_tfidf = tfidf[unrelated_doc_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc2 = \"As a homeless person, I need all the help I can get from the police.\"\n",
    "related_doc = example_doc2.lower().split()\n",
    "related_doc_dict = dictionary.doc2bow(related_doc)\n",
    "related_doc_tfidf = tfidf[related_doc_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tfidf weight of unrelated_doc: \n[]\n\nTfidf weight of related_doc: \n[(2, 0.34008989529272204), (65, 0.6809871315734689), (75, 0.6485332603275829)]\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tfidf weight of unrelated_doc: \\n{unrelated_doc_tfidf}\\n\")\n",
    "print(f\"Tfidf weight of related_doc: \\n{related_doc_tfidf}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing\n",
    "Use the tf-idf vector space to train LSI model (chain) with the bow_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to disk\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
    "    lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
    "\n",
    "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(lsi_model.print_topics(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LSI to test document\n",
    "Similarity queries; Sample unknown dataset related to topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import reddit_titles\n",
    "with open(\"./titles.txt\", \"r\") as f:\n",
    "    data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus] # bow_corpus as vector of tf-idf weights\n",
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)\n",
    "corpus_lsi = lsi_model[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi_model[corpus_tfidf]) # tf-idf weights of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dictionary.doc2bow(example_doc.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lsi = lsi_model[texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_lsi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[test_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_sp = []\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims:\n",
    "    doc_sp.append([doc_score, documents[doc_position]]) # documents = list of sentences; documents[doc_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sims[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_corpus, window=5, min_count=10, workers=4) # Omit: vector_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(['hello','world'], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['homeless']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vocab['homeless'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2 = KeyedVectors.load('word2vec.model', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = wv2.__getitem__(['homeless'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tf = []\n",
    "for ID, tf in sims:\n",
    "    sample_tf.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_tf, 'ob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(sample_tf), 'ob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = [item for item in sims]\n",
    "#print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cos_sim, '+') # Does not mean anything! Clusters; K-nearest neighbor?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common_list = c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(c.most_common_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ([[word] for word, count in c.most_common_list])\n",
    "y = ([[count] for word, count in c.most_common_list])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}