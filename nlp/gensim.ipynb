{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Gensim Library\n",
    "\n",
    "The following notebook will be an exercise in applying methods from the gensim library to the Blog Spot homeless blog corpus.\n",
    "\n",
    "The aim is to process data mined using the `blogspot_scraper` which can be used to model a classification system or aid in the statistical analysis of textual information. The hope is that it could useful for creating a framework that can be useful in predicting successful outcomes, or evaluating sentiment scores of posts, comments or writings of people dealing with homelessness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('blog_spot.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing and cleaning\n",
    "\n",
    "The text data is arranged as elements of a dataframe. Since this will be used as unsupervised training data. We can combine the writings of our sample into one series object to make mapping easier. Unless someone wants to explicitly point it out, I do not anticipate computational problems arising from this. Standard data processing and cleaning methods are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim .utils import simple_preprocess\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in function\n",
    "df1 = df.apply(lambda x: ','.join(x.astype(str)), axis=1).dropna()\n",
    "df2 = df1.apply(lambda x: remove_stopwords(x))\n",
    "df3 = df2.apply(lambda x: simple_preprocess(x, min_len=2))\n",
    "extra_stop_words = [\"http\", \"like\", \"nan\"] # extra_stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for i in extra_stop_words:\n",
    "    stop_words.append(i)\n",
    "\n",
    "df4 = df3.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "documents = df4.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save cleaned data to disk\"\"\"\n",
    "# TODO - try json format\n",
    "with open(\"documents.txt\", 'w') as file:\n",
    "    file.writelines(\"%s\\n\" % doc for doc in documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing processed data\n",
    "Below is a comparison of the before and after states of the text. The output in `documents` can now be converted into a bag-of-words (`bow`).\n",
    "\n",
    "Before:\n",
    "\n",
    "> '# Extracted from http://wanderingscribe.blogspot.com/\\n,#IWSG - JANUARY 2019 - CHECK IN - A NEW ME??? I CERTAINLY HOPE SO!\\n,Another one\\n,# Archived posts\\n,HOME VAN NEWSLETTER 6/12/16\\n,House Keys Not Handcuffs\\xa0\\n,The Homeless Finch Has Found Her Nest: Projects, Plans and Peace\\n'\n",
    "\n",
    "After:\n",
    "\n",
    "> ['extracted', 'wanderingscribe', 'blogspot', 'iwsg',...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign unique ID\n",
    "Now that we have a list representing the dictionary of words found in our corpus, we can create a dictionary of `{key: word, value: frequency_count}` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "# Create a frequency count for words in dataframe\n",
    "for text in df4:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Sample frequency count\n",
    "word = 'homeless'\n",
    "print(\"The word {} appears {} times in the corpus\".format(word, frequency[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words\n",
    "Using the `collections` module, we can see what the top 10 words by frequency count are.  We can also use this to further remove non-content words from the corpus if necessary. This is how I found that the word 'http' was everywhere in the corpus. I've removed it since, but it gives you an idea of non-content words you might have missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(frequency)\n",
    "c.most_common(10)"
   ]
  },
  {
   "source": [
    "> [('nan', 5232),\n",
    " ('homeless', 1971),\n",
    " ('people', 1803),\n",
    " ('night', 1048),\n",
    " ('time', 934),\n",
    " ('nightwatch', 860),\n",
    " ('know', 807),\n",
    " ('going', 770),\n",
    " ('shelter', 750),\n",
    " ('got', 687)]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Word appearing n times\n",
    "I thought it might be a good idea to find words appearing n number of times in the corpus. You can also do the exact opposite and find words that appear less than n times in the corpus and decide whether you want to keep those. This makes sense (as the Gensim docs point out) when you're using the library on massive datasets like the google news dataset - in a corpus of a billion words, you might want to decide that words appearing less than 5 times can be removed, for example."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output words appearing at least n times\n",
    "n = 10\n",
    "processed_corpus = [[token for token in text if frequency[token] > n] for text in df4]\n",
    "print(processed_corpus[:2])"
   ]
  },
  {
   "source": [
    "> [['blogspot', 'com', 'iwsg', 'january', 'check', 'new', 'certainly', 'hope', 'another', 'posts', 'home', 'van', 'newsletter', 'house', 'keys', 'homeless', 'finch', 'found', 'projects', 'plans', 'peace'], ['case', 'wondering', 'book', 'new', 'cover', 'th', 'november', 'cover', 'looks', 'sure', 'pink', 'white', 'time', 'actually', 'written', 'post', 'blog', 'figured', 'time', 'update', 'readers', 'past', 'year', 'bit', 'blogspot', 'com', 'home', 'van', 'newsletter', 'good', 'idea', 'homeless', 'finch', 'makes', 'first', 'rescue']]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary keys\n",
    "Now that we have a list of all the words in the corpus, we can create a dictionary to map any future comparison to. These will be assigned a unique token_id for every token or word in the corpus. What we'll end up with is a bag-of-words with unique identifiers for words in our entire corpus. Note the difference between `processed_corpus` and `bow_corpus`, where the former is a untokenized list of context words in our corpus. For this, we will use the built-in `corpora` module from the `gensim` library to make things easy.\n",
    "\n",
    "### Sample output:\n",
    "\n",
    "dictionary.token2id:\n",
    "> {'check': 0, 'home': 1, 'homeless': 2, 'hope': 3...\n",
    "\n",
    "bow_corpus:\n",
    "> [(2, 4), (3, 1), (4, 1), (5, 2), (6, 9)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create (token, tokenID)\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "# bow representation of the corpus; (tokenID, freq)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply doc2bow to example_doc\n",
    "Now we can test whether we can identify words present in an `example_doc` that are also in our `bow_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify for testing\n",
    "example_doc = \"it's a nice and sunny day outside, i'm so happy\"\n",
    "\n",
    "# Finds words in example_doc that appear within the context of our corpus\n",
    "example_bow = dictionary.doc2bow(example_doc.lower().split())\n",
    "example_bow\n",
    "\n",
    "if len(example_bow) == 0:\n",
    "    print(\"No context word found.\")\n",
    "else:\n",
    "    print(f\"The words: {[dictionary[tokenID] for tokenID, freq in example_bow]} are in our dictionary. No biggie.\")\n",
    "\n",
    "# TODO - Output (token, token_freq) in example_doc for words in corpora.Dictionary (processed_corpus)"
   ]
  },
  {
   "source": [
    "> The words: ['day', 'happy', 'nice'] are in our dictionary. No biggie.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "## TF-IDF weight\n",
    "In order to quantify the relationship of words in a document (and across documents), we'll create a word embeddings using `tf-idf` weights using the bow_corpus. We'll then transform `example_doc2` into a bag-of-words. This way, we will be able to determine which words in our `example_doc2` also appear in our training data, and actually measure both the frequency of those words in a document, and across documents. This is measured by it's tf-idf weight and helps answer the question: \"How relevant are words that appear in a document across a collection of documents\". The wikipedia article on tf-idf probably does a better job at explaining this. Later we will use a different algorithm (LSI) to calculate the similarity of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Initialize tf-idf model using bow_corpus\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "source": [
    "### bow representation to tf-idf weights\n",
    "This gives us a measure of the frequency of a word (i.e. assumed relevance) given a corpus of documents.\n",
    "\n",
    "Sample `tfidf[dictionary.doc2bow(sample)]` output:\n",
    "\n",
    "> [(0, 0.017154431730943658), (1, 0.020673564600620076), (2, 0.033339482607518046)..."
   ],
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with manual document\n",
    "We can test how well the model is able to determine an input's similiarity, measured in terms of tf-idf weights, by feeding it some text manually and seeing how it compares to the transformed corpus.\n",
    "\n",
    "The output gives you the `(dictionary[key], tf-idf_weight)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc1 = \"I am a student and I like to study natural language processing and computer science.\"\n",
    "unrelated_doc = example_doc1.lower().split()\n",
    "unrelated_doc_dict = dictionary.doc2bow(unrelated_doc)\n",
    "unrelated_doc_tfidf = tfidf[unrelated_doc_dict]\n",
    "\n",
    "example_doc2 = \"As a homeless person, I need all the help I can get from the police, instead of them harrassing me.\"\n",
    "related_doc = example_doc2.lower().split()\n",
    "related_doc_dict = dictionary.doc2bow(related_doc)\n",
    "related_doc_tfidf = tfidf[related_doc_dict]\n",
    "\n",
    "print(f\"Tfidf weight of unrelated_doc: \\n{unrelated_doc_tfidf}\\n\")\n",
    "print(f\"Tfidf weight of related_doc: \\n{related_doc_tfidf}\\n\")\n",
    "\n",
    "# TODO - create clean output"
   ]
  },
  {
   "source": [
    ">Tfidf weight of unrelated_doc: \n",
    ">[(494, 0.5792698834554537), (1630, 0.5662128359240098), (2361, 0.5863867550997339)]\n",
    ">\n",
    ">Tfidf weight of related_doc: \n",
    ">[(8, 0.15950015416535915), (191, 0.3193789464903235), (241, 0.30415827237259907), (353, 0.6306102318406809), (1234, 0.6183649975581706)]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to disk\n",
    "> TODO\n",
    "> \n",
    "> import os\n",
    "> import tempfile\n",
    "> \n",
    "> with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
    ">     lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
    "> \n",
    "> loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "> \n",
    "> os.unlink(tmp.name)"
   ]
  },
  {
   "source": [
    "### Applying the LSI (chain) model to the corpus.\n",
    "An interesting observation here is the difference in results obtained when applying LSI on top of either: a bag-of-words representation of the corpus (`bow_corpus`) or a tf-idf transformation (`corpus_tfidf`) on top of the bow_corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics to generate\n",
    "NUM_TOPICS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW -> LSI\n",
    "lsi_bow = models.LsiModel(bow_corpus, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "\n",
    "#print(f\"\\nTopics generated with BOW:\\n{lsi_bow.show_topics()}\")\n",
    "\n",
    "# BOW -> TF-IDF -> LSI \n",
    "tfidf_corpus = tfidf[bow_corpus] # bow_corpus as vector of tf-idf weights\n",
    "lsi_tfidf = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "\n",
    "#print(f\"\\nTopics generated with TF-IDF:\\n{lsi_tfidf.show_topics()}\")"
   ]
  },
  {
   "source": [
    "Aside from having to do some more clean up, it's interesting to note the difference in words included in the generated topic by the LSI model. I'd have to dig deeper as to why, but based on what I've understood, the difference could be coming from the factoring in of term frequencies across documents (the idf part) which effectively penalizes the model for words that occur too frequent in the corpus, but have no semantic meaning within a document.\n",
    "\n",
    "Considering that the corpus is comprised of homeless blogs for example, one has to ask whether the word `homeless` actually carries much meaning in the corpus? Remember that BOW simply does a frequency count of words in a document, but does not account for justifiably meaningless words (functional words) that appear too frequent across the collection of documents. TF-IDF weights does exactly that - it adjusts (penalizes) for words that are too common to bear actual meaning (context words) for the document."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Topics generated with BOW:\n",
    "> [(0, '0.411*\"homeless\" + 0.341*\"people\" + 0.257*\"nan\" + 0.180*\"city\" + 0.151*\"night\" + 0.148*\"time\" + 0.146*\"shelter\" + 0.126*\"know\" + 0.125*\"life\" > + 0.116*\"going\"'), (1, '0.805*\"nan\" + -0.270*\"homeless\" + -0.178*\"city\" + -0.142*\"police\" + -0.120*\"people\" + -0.107*\"pam\" + 0.100*\"night\" + > 0.099*\"nightwatch\" + -0.084*\"homelessness\" + -0.082*\"law\"')]\n",
    "> \n",
    "> Topics generated with tf-idf weights:\n",
    "> [(0, '0.155*\"homeless\" + 0.149*\"people\" + 0.136*\"night\" + 0.122*\"nightwatch\" + 0.109*\"shelter\" + 0.105*\"time\" + 0.104*\"seattle\" + 0.102*\"going\" + > 0.098*\"know\" + 0.098*\"city\"'), (1, '-0.229*\"night\" + -0.223*\"shelter\" + -0.212*\"nightwatch\" + -0.184*\"seattle\" + -0.178*\"women\" + -0.157*\"bar\" + > 0.150*\"life\" + -0.115*\"city\" + -0.109*\"operation\" + 0.104*\"justice\"')]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Similarity Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ok, so let's see if I can wrap my head around this properly. After creating an lsi vector of the bow representation of our corpus, which was converted into tf-idf weights for reasons stated above, we will now create a similarity matrix that will serve as our index. This will allow us to compare a new `test_doc`, so that we can find out how similar it is to the documents in our corpus.\n",
    "\n",
    "And because we now have something to index with, we can convert our `test_doc` into it's own bow representation. These would be words in `test_doc` that are found in the `dictionary` we created earlier using the original corpus, in the form `tuple(token_id, frequency_count)`. We can then apply the same transformations to the bow representation.\n",
    "\n",
    "Again, I'm trying to see how different the effect of transforming a bow vector directly into an LSI space, versus a tf-idf transformation **after** a bow conversion. The small difference probably does account for the comparison across documents, so I'll probably stick to that model. Again, I'm not a 100% on this.\n",
    "\n",
    "After this, we'll have an LSI representation of the test_doc. We can then perform a similarity query against our corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.MatrixSimilarity(lsi_tfidf[bow_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = \"the mario brothers are in town\"\n",
    "\n",
    "# TODO - save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test_doc to bow representation\n",
    "test_doc_bow = dictionary.doc2bow(test_doc.lower().split())"
   ]
  },
  {
   "source": [
    ">[(41, 1),\n",
    "> (184, 1),\n",
    "> (202, 1),\n",
    "> (353, 1),\n",
    "> (629, 1),\n",
    "> (743, 1),\n",
    "> (848, 1),\n",
    "> (877, 1),\n",
    "> (1194, 1),\n",
    "> (1523, 1),\n",
    "> (1575, 2),\n",
    "> (1716, 1),\n",
    "> (1808, 1),\n",
    "> (1989, 1),\n",
    "> (2039, 1)]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test_doc_bow to an lsi vector\n",
    "test_lsi = lsi_tfidf[test_doc_bow]\n",
    "\n",
    "# Transform test_doc_bow to a bow vector\n",
    "test_bow = lsi_bow[test_doc_bow]\n",
    "\n",
    "print(f\"test_lsi: {test_lsi}\\n\")\n",
    "#print(f\"test_bow: {test_bow}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sims = index[test_lsi]\n",
    "print(f\"Sample of cosine similarity scores:\\n\\n{list(enumerate(sims))[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Descending order\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "for doc_position, doc_score in sims[:10]:\n",
    "    print(doc_score, documents[doc_position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_corpus, window=5, min_count=10, workers=4) # Omit: vector_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(['hello','world'], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['homeless']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vocab['homeless'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2 = KeyedVectors.load('word2vec.model', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = wv2.__getitem__(['homeless'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tf = []\n",
    "for ID, tf in sims:\n",
    "    sample_tf.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_tf, 'ob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(sample_tf), 'ob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = [item for item in sims]\n",
    "#print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cos_sim, '+') # Does not mean anything! Clusters; K-nearest neighbor?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common_list = c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(c.most_common_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ([[word] for word, count in c.most_common_list])\n",
    "y = ([[count] for word, count in c.most_common_list])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}